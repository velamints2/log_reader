#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœºå™¨äººæ—¥å¿—åˆ†æç³»ç»Ÿ - JSON API æœåŠ¡
ç”¨äºå¤§æ¨¡å‹é›†æˆçš„ç²¾ç®€ JSON æ•°æ®æ¥å£ï¼ˆæ— å¯è§†åŒ–ï¼Œä»…ä¼ é€’ä¿¡æ¯ï¼‰
"""

import os
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from comprehensive_robot_analyzer import ComprehensiveRobotAnalyzer
from complaint_analyzer import ComplaintAnalyzer
from historical_trace_analyzer import HistoricalTraceAnalyzer
from config import LOG_DIRECTORY, TEMP_REPORTS_DIRECTORY


def analyze_logs_json(log_directory: str = LOG_DIRECTORY) -> Dict[str, Any]:
    """
    ç»¼åˆæ—¥å¿—åˆ†æ - è¿”å› JSON æ ¼å¼ç»“æœ
    
    Args:
        log_directory: æ—¥å¿—ç›®å½•è·¯å¾„
        
    Returns:
        åŒ…å«åˆ†æç»“æœçš„å­—å…¸
    """
    try:
        # æ‰§è¡Œç»¼åˆåˆ†æ
        analyzer = ComprehensiveRobotAnalyzer(log_directory)
        analyzer.analyze_all_logs()
        report = analyzer.generate_comprehensive_report()
        
        # ç²¾ç®€å…³é”®ä¿¡æ¯
        result = {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "analysis_summary": report.get("analysis_summary", {}),
            "anomaly_summary": report.get("anomaly_summary", {}),
            "key_findings": {
                "total_log_files": report.get("analysis_summary", {}).get("total_log_files", 0),
                "total_anomalies": report.get("analysis_summary", {}).get("total_anomalies", 0),
                "total_task_segments": report.get("analysis_summary", {}).get("total_task_segments", 0),
                "critical_anomalies": [
                    a for a in report.get("anomalies", [])
                    if a.get("severity") in ["critical", "high"]
                ][:10]  # åªè¿”å›å‰10ä¸ªå…³é”®å¼‚å¸¸
            }
        }
        return result
    except Exception as e:
        return {
            "status": "error",
            "message": f"åˆ†æå¤±è´¥: {str(e)}"
        }


def historical_trace_json(log_directory: str = LOG_DIRECTORY) -> Dict[str, Any]:
    """
    å†å²è¿½æº¯åˆ†æ - è¿”å› JSON æ ¼å¼ç»“æœ
    
    Args:
        log_directory: æ—¥å¿—ç›®å½•è·¯å¾„
        
    Returns:
        åŒ…å«å†å²è¿½æº¯ç»“æœçš„å­—å…¸
    """
    try:
        analyzer = HistoricalTraceAnalyzer(log_directory)
        analyzer.analyze_all_logs()
        report = analyzer.generate_trace_report()
        
        # ç²¾ç®€å…³é”®ä¿¡æ¯
        result = {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "trace_summary": {
                "total_task_segments": len(report.get("task_timeline", [])),
                "time_span": report.get("analysis_metadata", {}).get("time_span"),
                "earliest_time": report.get("analysis_metadata", {}).get("earliest_time"),
                "latest_time": report.get("analysis_metadata", {}).get("latest_time"),
            },
            "task_timeline": report.get("task_timeline", [])[:20],  # ä»…è¿”å›å‰20æ¡
            "system_state_transitions": report.get("system_state_transitions", [])[:20],
            "anomaly_timeline": report.get("anomaly_timeline", [])[:20]
        }
        return result
    except Exception as e:
        return {
            "status": "error",
            "message": f"å†å²è¿½æº¯å¤±è´¥: {str(e)}"
        }


def complaint_analysis_json(
    log_directory: str = LOG_DIRECTORY,
    complaint_time_str: Optional[str] = None
) -> Dict[str, Any]:
    """
    æŠ•è¯‰åˆ†æ - è¿”å› JSON æ ¼å¼ç»“æœ
    
    Args:
        log_directory: æ—¥å¿—ç›®å½•è·¯å¾„
        complaint_time_str: æŠ•è¯‰æ—¶é—´ï¼Œæ ¼å¼ "YYYY-MM-DD HH:MM:SS"
        
    Returns:
        åŒ…å«æŠ•è¯‰åˆ†æç»“æœçš„å­—å…¸
    """
    try:
        if not complaint_time_str:
            return {
                "status": "error",
                "message": "ç¼ºå°‘å‚æ•°: complaint_time (æ ¼å¼: YYYY-MM-DD HH:MM:SS)"
            }
        
        complaint_time = datetime.strptime(complaint_time_str, "%Y-%m-%d %H:%M:%S")
        
        analyzer = ComplaintAnalyzer(log_directory)
        analyzer.analyze_all_logs()
        report = analyzer.generate_complaint_report(complaint_time)
        
        # ç²¾ç®€å…³é”®ä¿¡æ¯
        result = {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "complaint_time": complaint_time_str,
            "analysis_window": {
                "start": (complaint_time - datetime.timedelta(minutes=10)).isoformat(),
                "end": (complaint_time + datetime.timedelta(minutes=10)).isoformat()
            },
            "complaint_summary": report.get("complaint_summary", {}),
            "pre_complaint_events": report.get("pre_complaint_events", [])[:15],
            "complaint_time_events": report.get("complaint_time_events", [])[:15],
            "post_complaint_events": report.get("post_complaint_events", [])[:15],
            "root_cause_analysis": report.get("root_cause_analysis", {}),
            "key_findings": report.get("key_findings", [])
        }
        return result
    except ValueError as e:
        return {
            "status": "error",
            "message": f"æ—¶é—´æ ¼å¼é”™è¯¯: {str(e)}"
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"æŠ•è¯‰åˆ†æå¤±è´¥: {str(e)}"
        }


def anomaly_summary_json(log_directory: str = LOG_DIRECTORY) -> Dict[str, Any]:
    """
    å¼‚å¸¸æ±‡æ€» - è¿”å› JSON æ ¼å¼ç»“æœï¼ˆæ— è¯¦ç»†æ—¥å¿—å†…å®¹ï¼‰
    
    Args:
        log_directory: æ—¥å¿—ç›®å½•è·¯å¾„
        
    Returns:
        åŒ…å«å¼‚å¸¸ç»Ÿè®¡çš„å­—å…¸
    """
    try:
        analyzer = ComprehensiveRobotAnalyzer(log_directory)
        analyzer.analyze_all_logs()
        report = analyzer.generate_comprehensive_report()
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»ç»Ÿè®¡
        anomalies = report.get("anomalies", [])
        
        severity_count = {
            "critical": len([a for a in anomalies if a.get("severity") == "critical"]),
            "high": len([a for a in anomalies if a.get("severity") == "high"]),
            "medium": len([a for a in anomalies if a.get("severity") == "medium"]),
            "low": len([a for a in anomalies if a.get("severity") == "low"])
        }
        
        # æŒ‰ç±»å‹åˆ†ç±»
        type_count = {}
        for anomaly in anomalies:
            atype = anomaly.get("type", "unknown")
            type_count[atype] = type_count.get(atype, 0) + 1
        
        # æŒ‰æ–‡ä»¶åˆ†ç±»
        file_count = {}
        for anomaly in anomalies:
            fname = anomaly.get("file", "unknown")
            file_count[fname] = file_count.get(fname, 0) + 1
        
        result = {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "total_anomalies": len(anomalies),
            "severity_distribution": severity_count,
            "type_distribution": type_count,
            "file_distribution": file_count,
            "top_anomalies": [
                {
                    "type": a.get("type"),
                    "severity": a.get("severity"),
                    "file": a.get("file"),
                    "time": a.get("time"),
                    "message": a.get("message", "")[:100]  # ä»…è¿”å›å‰100å­—
                }
                for a in anomalies[:30]  # ä»…è¿”å›å‰30æ¡
            ]
        }
        return result
    except Exception as e:
        return {
            "status": "error",
            "message": f"å¼‚å¸¸æ±‡æ€»å¤±è´¥: {str(e)}"
        }


def log_files_info_json(log_directory: str = LOG_DIRECTORY) -> Dict[str, Any]:
    """
    æ—¥å¿—æ–‡ä»¶ä¿¡æ¯ - è¿”å› JSON æ ¼å¼ç»“æœ
    
    Args:
        log_directory: æ—¥å¿—ç›®å½•è·¯å¾„
        
    Returns:
        åŒ…å«æ—¥å¿—æ–‡ä»¶ä¿¡æ¯çš„å­—å…¸
    """
    try:
        if not os.path.exists(log_directory):
            return {
                "status": "error",
                "message": f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_directory}"
            }
        
        log_files = []
        total_size = 0
        
        for filename in os.listdir(log_directory):
            filepath = os.path.join(log_directory, filename)
            if os.path.isfile(filepath):
                stat = os.stat(filepath)
                size = stat.st_size
                total_size += size
                
                log_files.append({
                    "name": filename,
                    "size_bytes": size,
                    "size_kb": round(size / 1024, 2),
                    "modified_time": datetime.fromtimestamp(stat.st_mtime).isoformat()
                })
        
        # æŒ‰å¤§å°æ’åº
        log_files.sort(key=lambda x: x["size_bytes"], reverse=True)
        
        result = {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "log_directory": log_directory,
            "total_files": len(log_files),
            "total_size_bytes": total_size,
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "log_files": log_files
        }
        return result
    except Exception as e:
        return {
            "status": "error",
            "message": f"è·å–æ—¥å¿—æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}"
        }


def system_health_json(log_directory: str = LOG_DIRECTORY) -> Dict[str, Any]:
    """
    ç³»ç»Ÿå¥åº·è¯„ä¼° - è¿”å› JSON æ ¼å¼ç»“æœ
    
    Args:
        log_directory: æ—¥å¿—ç›®å½•è·¯å¾„
        
    Returns:
        åŒ…å«ç³»ç»Ÿå¥åº·çŠ¶æ€çš„å­—å…¸
    """
    try:
        analyzer = ComprehensiveRobotAnalyzer(log_directory)
        analyzer.analyze_all_logs()
        report = analyzer.generate_comprehensive_report()
        
        # è®¡ç®—å¥åº·å¾—åˆ† (0-100)
        anomalies = report.get("anomalies", [])
        total_anomalies = len(anomalies)
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦è®¡ç®—åŠ æƒå¾—åˆ†
        critical_count = len([a for a in anomalies if a.get("severity") == "critical"])
        high_count = len([a for a in anomalies if a.get("severity") == "high"])
        medium_count = len([a for a in anomalies if a.get("severity") == "medium"])
        
        health_score = max(0, 100 - critical_count * 30 - high_count * 15 - medium_count * 5)
        
        # åˆ¤æ–­å¥åº·çŠ¶æ€
        if health_score >= 90:
            status = "excellent"
            status_cn = "ä¼˜ç§€"
        elif health_score >= 70:
            status = "good"
            status_cn = "è‰¯å¥½"
        elif health_score >= 50:
            status = "fair"
            status_cn = "ä¸€èˆ¬"
        elif health_score >= 30:
            status = "poor"
            status_cn = "è¾ƒå·®"
        else:
            status = "critical"
            status_cn = "ä¸¥é‡"
        
        result = {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "health_score": health_score,
            "health_status": status,
            "health_status_cn": status_cn,
            "anomaly_breakdown": {
                "critical": critical_count,
                "high": high_count,
                "medium": medium_count,
                "low": len([a for a in anomalies if a.get("severity") == "low"]),
                "total": total_anomalies
            },
            "key_issues": [
                {
                    "severity": a.get("severity"),
                    "type": a.get("type"),
                    "count": 1,
                    "message": a.get("message", "")[:80]
                }
                for a in anomalies[:10]
            ],
            "recommendations": generate_recommendations(health_score, critical_count, high_count)
        }
        return result
    except Exception as e:
        return {
            "status": "error",
            "message": f"ç³»ç»Ÿå¥åº·è¯„ä¼°å¤±è´¥: {str(e)}"
        }


def generate_recommendations(health_score: float, critical_count: int, high_count: int) -> List[str]:
    """ç”Ÿæˆå»ºè®®åˆ—è¡¨"""
    recommendations = []
    
    if critical_count > 0:
        recommendations.append(f"âŒ å‘ç° {critical_count} ä¸ªä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ç«‹å³å¤„ç†")
    
    if high_count > 0:
        recommendations.append(f"âš ï¸ å‘ç° {high_count} ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜ï¼Œå»ºè®®å°½å¿«ä¿®å¤")
    
    if health_score < 50:
        recommendations.append("ğŸ”§ ç³»ç»ŸçŠ¶æ€æ¬ ä½³ï¼Œå»ºè®®è¿›è¡Œå…¨é¢è¯Šæ–­å’Œç»´æŠ¤")
    elif health_score < 70:
        recommendations.append("ğŸ“Š ç³»ç»Ÿæœ‰æ”¹è¿›ç©ºé—´ï¼Œå»ºè®®å…³æ³¨é«˜ä¼˜å…ˆçº§é—®é¢˜")
    else:
        recommendations.append("âœ… ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œè¯·ç»§ç»­ç›‘æ§")
    
    return recommendations


if __name__ == "__main__":
    # æµ‹è¯•è„šæœ¬
    print("=" * 60)
    print("JSON API æœåŠ¡æµ‹è¯•")
    print("=" * 60)
    
    print("\n1. æ—¥å¿—åˆ†æç»“æœ:")
    result = analyze_logs_json()
    print(json.dumps(result, ensure_ascii=False, indent=2)[:500] + "...")
    
    print("\n2. å†å²è¿½æº¯ç»“æœ:")
    result = historical_trace_json()
    print(json.dumps(result, ensure_ascii=False, indent=2)[:500] + "...")
    
    print("\n3. å¼‚å¸¸æ±‡æ€»ç»“æœ:")
    result = anomaly_summary_json()
    print(json.dumps(result, ensure_ascii=False, indent=2))
    
    print("\n4. æ—¥å¿—æ–‡ä»¶ä¿¡æ¯:")
    result = log_files_info_json()
    print(json.dumps(result, ensure_ascii=False, indent=2)[:500] + "...")
    
    print("\n5. ç³»ç»Ÿå¥åº·è¯„ä¼°:")
    result = system_health_json()
    print(json.dumps(result, ensure_ascii=False, indent=2))
