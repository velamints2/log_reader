from flask import Flask
from flask_cors import CORS
from flask import jsonify
from flask import request
from flask import send_from_directory, send_file
import sys
import os
import json
from datetime import datetime, timedelta

# æ·»åŠ æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥é…ç½®æ–‡ä»¶
from config import OPENAI_API_KEY, BASE_URL, LOG_DIRECTORY, TEMP_REPORTS_DIRECTORY
from config import API_KEY, API_BASE_URL, API_MODEL, MAX_TOKENS, TEMPERATURE, REQUEST_TIMEOUT
import requests
import time
import re

# å¯¼å…¥åˆ†æå™¨
from complete_robot_log_analyzer import CompleteRobotLogAnalyzer
from config import REPORTS_DIRECTORY

# å¯¼å…¥æ™ºèƒ½æ—¥å¿—è¯Šæ–­Agent
from log_agent import LogDiagnosticAgent

# å…¨å±€è®¾ç½®å­˜å‚¨å¯¹è±¡
settings_storage = {}

# å‰ç«¯é™æ€æ–‡ä»¶ç›®å½•
FRONTEND_DIR = os.path.join(os.path.dirname(__file__), "..", "frontend")

app = Flask(__name__)
CORS(app)

@app.route("/api/status", methods=["GET"])
def api_status():
    return jsonify({
        "status": "success",
        "message": "Backend is running"
    })

@app.route("/api/test", methods=["GET"])
def test_api():
    """æµ‹è¯•APIè¿æ¥"""
    return jsonify({
        "status": "success",
        "message": "APIè¿æ¥æ­£å¸¸",
        "timestamp": datetime.now().isoformat()
    })

@app.route("/api/logs", methods=["GET"])
def get_logs():
    """è·å–æ—¥å¿—æ–‡ä»¶åˆ—è¡¨"""
    log_directory = LOG_DIRECTORY
    if not os.path.exists(log_directory):
        return jsonify({"error": "æ—¥å¿—ç›®å½•ä¸å­˜åœ¨"}), 404
    
    log_files = []
    for filename in os.listdir(log_directory):
        file_path = os.path.join(log_directory, filename)
        if os.path.isfile(file_path):
            size = os.path.getsize(file_path)
            log_files.append({
                "name": filename,
                "size": size,
                "type": "log"
            })
    
    return jsonify({
        "status": "success",
        "log_files": log_files,
        "total_count": len(log_files)
    })

@app.route("/api/settings", methods=["GET"])
def get_settings():
    return jsonify({
        "status": "success",
        "settings": {
            "api_provider": settings_storage.get("api_provider", "openai"),
            "api_key_configured": bool(settings_storage.get("api_key") or OPENAI_API_KEY),
            "base_url": settings_storage.get("base_url", BASE_URL)
        }
    })

@app.route("/api/settings", methods=["POST"])
def update_settings():
    data = request.get_json() or {}
    for key in ["api_provider", "api_key", "base_url", "model"]:
        if key in data:
            settings_storage[key] = data[key]
    return jsonify({"status": "success", "settings": settings_storage})

@app.route("/api/analyze", methods=["POST"])
def analyze_logs():
    data = request.get_json() or {}
    log_directory = data.get("log_directory", LOG_DIRECTORY)
    enable_ai = bool(data.get("enable_ai", True))  # é»˜è®¤å¯ç”¨AI
    report_type = data.get("report_type", "enhanced")  # é»˜è®¤ä¸ºå¢å¼ºåˆ†æ
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    if not os.path.exists(log_directory):
        return jsonify({
            "status": "error",
            "message": f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_directory}"
        }), 400
    
    # æ ¹æ®æŠ¥å‘Šç±»å‹é€‰æ‹©åˆ†æå™¨
    if report_type == "basic":
        # ä½¿ç”¨ç®€å•åˆ†æå™¨
        return analyze_with_basic_analyzer(data)
    else:
        # ä½¿ç”¨ç»¼åˆåˆ†æå™¨è¿›è¡Œæ·±åº¦åˆ†æ
        return analyze_with_comprehensive_analyzer(data)

def analyze_with_basic_analyzer(data):
    """åŸºç¡€åˆ†æ"""
    log_directory = data.get("log_directory", LOG_DIRECTORY)
    analyzer = CompleteRobotLogAnalyzer(log_directory)
    result = analyzer.save_reports(TEMP_REPORTS_DIRECTORY)
    
    return jsonify({
        "status": "success",
        "report_id": result["report_id"],
        "paths": {
            "json": result["json_path"],
            "txt": result["txt_path"]
        },
        "analysis_type": "basic",
        "message": "åŸºç¡€åˆ†æå®Œæˆ"
    })

def analyze_with_comprehensive_analyzer(data):
    """ç»¼åˆåˆ†æ"""
    from comprehensive_robot_analyzer import ComprehensiveRobotAnalyzer
    from complaint_analyzer import ComplaintAnalyzer
    from historical_trace_analyzer import HistoricalTraceAnalyzer
    from enhanced_detailed_report_generator import EnhancedDetailedReportGenerator
    from deepseek_enhanced_report_generator import DeepSeekEnhancedReportGenerator
    
    log_directory = data.get("log_directory", LOG_DIRECTORY)
    complaint_time_str = data.get("complaint_time")
    output_dir = TEMP_REPORTS_DIRECTORY
    
    try:
        # ç”Ÿæˆå”¯ä¸€çš„æŠ¥å‘ŠID
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_prefix = f"comprehensive_report_{timestamp}"
        
        # 1. ç»¼åˆæ—¥å¿—åˆ†æ
        print("ğŸ” å¼€å§‹ç»¼åˆæ—¥å¿—åˆ†æ...")
        comprehensive_analyzer = ComprehensiveRobotAnalyzer(log_directory)
        comprehensive_analyzer.analyze_all_logs()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        comprehensive_report = comprehensive_analyzer.generate_comprehensive_report()
        comprehensive_report_path = os.path.join(output_dir, f"{report_prefix}.json")
        comprehensive_analyzer.save_report(comprehensive_report, comprehensive_report_path)
        
        # 2. æŠ•è¯‰åˆ†æï¼ˆå¦‚æœæœ‰æŠ•è¯‰æ—¶é—´ï¼‰
        complaint_result = None
        if complaint_time_str:
            try:
                complaint_time = datetime.strptime(complaint_time_str, "%Y-%m-%d %H:%M:%S")
                print("ğŸ” å¼€å§‹æŠ•è¯‰åˆ†æ...")
                complaint_analyzer = ComplaintAnalyzer(log_directory)
                complaint_analyzer.analyze_all_logs()
                
                complaint_report = complaint_analyzer.generate_complaint_report(
                    complaint_time, 
                    os.path.join(output_dir, f"complaint_report_{timestamp}.json")
                )
                complaint_result = complaint_report
            except Exception as e:
                print(f"âš ï¸ æŠ•è¯‰åˆ†æå¤±è´¥: {e}")
        
        # 3. å†å²è¿½æº¯åˆ†æ
        print("ğŸ” å¼€å§‹å†å²è¿½æº¯åˆ†æ...")
        historical_analyzer = HistoricalTraceAnalyzer(log_directory)
        historical_analyzer.analyze_all_logs()
        historical_report_path = os.path.join(output_dir, f"historical_trace_report_{timestamp}.json")
        historical_report = historical_analyzer.generate_trace_report(output_file=historical_report_path)
        
        # 4. é›†æˆæ‰€æœ‰åˆ†æç»“æœ
        print("ğŸ”„ é›†æˆåˆ†æç»“æœ...")
        integrated_report = {
            "report_metadata": {
                "report_id": report_prefix,
                "generated_at": datetime.now().isoformat(),
                "log_directory": log_directory,
                "analysis_type": "comprehensive",
                "enable_ai": True
            },
            "comprehensive_analysis": comprehensive_report,
            "historical_trace": historical_report,
            "complaint_analysis": complaint_result,
            "integrated_summary": {
                "total_log_files": comprehensive_report.get("analysis_summary", {}).get("total_log_files", 0),
                "total_anomalies": comprehensive_report.get("analysis_summary", {}).get("total_anomalies", 0),
                "total_task_segments": comprehensive_report.get("analysis_summary", {}).get("total_task_segments", 0),
                "analysis_timestamp": datetime.now().isoformat()
            }
        }
        
        # ä¿å­˜é›†æˆæŠ¥å‘Š
        integrated_path = os.path.join(output_dir, f"integrated_report_{timestamp}.json")
        with open(integrated_path, 'w', encoding='utf-8') as f:
            json.dump(integrated_report, f, ensure_ascii=False, indent=2, default=str)
        
        # 5. ç”Ÿæˆå¢å¼ºHTMLæŠ¥å‘Š
        print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š...")
        html_report_path = None
        deepseek_html_report_path = None
        
        try:
            # è·å–æŠ¥å‘Šç±»å‹
            report_type = data.get('report_type', 'enhanced')
            
            # ç”Ÿæˆæ ‡å‡†å¢å¼ºæŠ¥å‘Š
            if report_type in ['enhanced', 'comprehensive']:
                html_generator = EnhancedDetailedReportGenerator(integrated_path)
                html_report_path = os.path.join(output_dir, f"enhanced_detailed_report_{timestamp}.html")
                html_generator.generate_detailed_report(html_report_path)
                
                # åŒæ—¶ä¿å­˜åˆ°reportsç›®å½•ä¾›ä¸‹è½½
                reports_html_path = os.path.join(REPORTS_DIRECTORY, f"enhanced_detailed_report_{timestamp}.html")
                html_generator.generate_detailed_report(reports_html_path)
            
            # ç”ŸæˆDeepSeekå¢å¼ºç‰ˆæŠ¥å‘Š
            if report_type in ['deepseek_enhanced', 'comprehensive']:
                print("ğŸ¤– ç”ŸæˆDeepSeek AIå¢å¼ºæŠ¥å‘Š...")
                deepseek_generator = DeepSeekEnhancedReportGenerator(integrated_path)
                deepseek_html_report_path = os.path.join(output_dir, f"deepseek_enhanced_report_{timestamp}.html")
                deepseek_generator.generate_detailed_report(deepseek_html_report_path)
                
                # åŒæ—¶ä¿å­˜åˆ°reportsç›®å½•ä¾›ä¸‹è½½
                deepseek_reports_path = os.path.join(REPORTS_DIRECTORY, f"deepseek_enhanced_report_{timestamp}.html")
                deepseek_generator.generate_detailed_report(deepseek_reports_path)
            
        except Exception as e:
            print(f"âš ï¸ HTMLæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        
        # æ„å»ºç»“æœå“åº”
        result_paths = {
            "json": integrated_path,
            "html": html_report_path,
            "deepseek_html": deepseek_html_report_path,
            "comprehensive_json": comprehensive_report_path,
            "historical_json": historical_report_path
        }
        
        if complaint_result:
            result_paths["complaint_json"] = os.path.join(output_dir, f"complaint_report_{timestamp}.json")
        
        return jsonify({
            "status": "success",
            "report_id": report_prefix,
            "analysis_type": "comprehensive",
            "message": "ç»¼åˆåˆ†æå®Œæˆ",
            "summary": integrated_report["integrated_summary"],
            "paths": result_paths,
            "analysis_details": {
                "log_files_analyzed": comprehensive_report.get("analysis_summary", {}).get("total_log_files", 0),
                "anomalies_detected": comprehensive_report.get("analysis_summary", {}).get("total_anomalies", 0),
                "task_segments_found": comprehensive_report.get("analysis_summary", {}).get("total_task_segments", 0),
                "ai_enhanced": True
            }
        })
        
    except Exception as e:
        print(f"âŒ ç»¼åˆåˆ†æå¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": f"åˆ†æå¤±è´¥: {str(e)}"
        }), 500

@app.route("/api/reports", methods=["GET"])
def list_reports():
    """åˆ—å‡ºæ‰€æœ‰æŠ¥å‘Šæ–‡ä»¶ï¼ŒåŒ…æ‹¬reportså’Œtemp_reportsç›®å½•"""
    os.makedirs(REPORTS_DIRECTORY, exist_ok=True)
    os.makedirs(TEMP_REPORTS_DIRECTORY, exist_ok=True)
    
    items = []
    
    # æ‰«æreportsç›®å½•
    for name in os.listdir(REPORTS_DIRECTORY):
        path = os.path.join(REPORTS_DIRECTORY, name)
        if os.path.isfile(path):
            size = os.path.getsize(path)
            file_ext = os.path.splitext(name)[1].lower()
            
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šç±»å‹
            if file_ext == '.html':
                file_type = "html"
            elif file_ext == '.json':
                file_type = "json"
            elif file_ext == '.txt':
                file_type = "text"
            else:
                file_type = "other"
            
            items.append({
                "id": os.path.splitext(name)[0],
                "name": name,
                "type": file_type,
                "size": f"{size} B",
                "path": f"./reports/{name}"
            })
    
    # æ‰«ætemp_reportsç›®å½•
    for name in os.listdir(TEMP_REPORTS_DIRECTORY):
        path = os.path.join(TEMP_REPORTS_DIRECTORY, name)
        if os.path.isfile(path):
            size = os.path.getsize(path)
            file_ext = os.path.splitext(name)[1].lower()
            
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šç±»å‹
            if file_ext == '.html':
                file_type = "html"
            elif file_ext == '.json':
                file_type = "json"
            elif file_ext == '.txt':
                file_type = "text"
            else:
                file_type = "other"
            
            items.append({
                "id": os.path.splitext(name)[0],
                "name": name,
                "type": file_type,
                "size": f"{size} B",
                "path": f"./temp_reports/{name}"
            })
    
    # æŒ‰æ–‡ä»¶åæ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
    items.sort(key=lambda x: x["name"], reverse=True)
    
    return jsonify(items)

@app.route("/api/report", methods=["GET"])
def serve_report():
    """æä¾›æŠ¥å‘Šæ–‡ä»¶æœåŠ¡"""
    path = request.args.get("path")
    if not path:
        return jsonify({"error": "ç¼ºå°‘è·¯å¾„å‚æ•°"}), 400
    
    # è§£ç è·¯å¾„
    import urllib.parse
    decoded_path = urllib.parse.unquote(path)
    
    # å¤„ç†ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    if decoded_path.startswith('./'):
        # ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        decoded_path = os.path.join(project_root, decoded_path[2:])
    elif not os.path.isabs(decoded_path):
        # å…¶ä»–ç›¸å¯¹è·¯å¾„ï¼Œä¹Ÿç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        decoded_path = os.path.join(project_root, decoded_path)
    
    # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è·¯å¾„åœ¨å…è®¸çš„ç›®å½•å†…
    allowed_dirs = [
        os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "reports"),
        os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "temp_reports")
    ]
    
    if not any(decoded_path.startswith(allowed_dir) for allowed_dir in allowed_dirs):
        return jsonify({"error": "è®¿é—®è·¯å¾„ä¸è¢«å…è®¸"}), 403
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(decoded_path):
        return jsonify({"error": f"æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {decoded_path}"}), 404
    
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åè¿”å›ä¸åŒçš„å†…å®¹ç±»å‹
    file_extension = os.path.splitext(decoded_path)[1].lower()
    
    if file_extension == '.html':
        # è¿”å›HTMLæ–‡ä»¶
        return send_file(decoded_path)
    elif file_extension == '.json':
        # è¿”å›JSONæ–‡ä»¶å†…å®¹
        try:
            with open(decoded_path, 'r', encoding='utf-8') as f:
                json_content = json.load(f)
            return jsonify(json_content)
        except Exception as e:
            return jsonify({"error": f"è¯»å–JSONæ–‡ä»¶å¤±è´¥: {str(e)}"}), 500
    else:
        # å…¶ä»–æ–‡ä»¶ç±»å‹ï¼Œå°è¯•ä½œä¸ºæ™®é€šæ–‡ä»¶å‘é€
        return send_file(decoded_path)


def _parse_timestamp_from_line(line: str):
    """å°è¯•ä»ä¸€è¡Œæ—¥å¿—ä¸­è§£æå¸¸è§çš„æ—¶é—´æˆ³æ ¼å¼ï¼Œè¿”å› datetime æˆ– Noneã€‚"""
    # å¸¸è§æ ¼å¼ï¼š
    # 1. 2025-10-12 00:00:00:004 (æ¯«ç§’ç”¨å†’å·åˆ†éš”ï¼Œç³»ç»Ÿæ—¥å¿—å¸¸è§æ ¼å¼)
    # 2. 2025-11-30 14:30:00 æˆ– 2025-11-30T14:30:00
    # 3. 2025/11/30 14:30:00
    # 4. [2025-11-30 14:30:00]
    patterns = [
        # æ¯«ç§’ç”¨å†’å·åˆ†éš”: 2025-10-12 00:00:00:004
        (r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}):\d+", '%Y-%m-%d %H:%M:%S'),
        # æ ‡å‡†æ ¼å¼: 2025-11-30 14:30:00.123 æˆ– 2025-11-30 14:30:00
        (r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})", '%Y-%m-%d %H:%M:%S'),
        # ISOæ ¼å¼: 2025-11-30T14:30:00
        (r"(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})", None),  # ä½¿ç”¨ fromisoformat
        # å¸¦æ–¹æ‹¬å·: [2025-11-30 14:30:00]
        (r"\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\]", '%Y-%m-%d %H:%M:%S'),
        # æ–œæ æ ¼å¼: 2025/11/30 14:30:00
        (r"(\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2})", '%Y/%m/%d %H:%M:%S'),
    ]
    for pattern, fmt in patterns:
        m = re.search(pattern, line)
        if m:
            s = m.group(1)
            try:
                if fmt is None:
                    return datetime.fromisoformat(s)
                return datetime.strptime(s, fmt)
            except Exception:
                continue
    return None


def extract_logs_around_time(issue_time_str: str, window_min: int = 10, max_lines: int = 1000):
    """ä» `LOG_DIRECTORY` ä¸­æå–ä»¥ issue_time ä¸ºä¸­å¿ƒã€å‰å window_min åˆ†é’Ÿçš„æ—¥å¿—è¡Œã€‚
    è¿”å› (joined_lines, error_message_or_None)
    """
    try:
        issue_time = datetime.strptime(issue_time_str, "%Y-%m-%d %H:%M:%S")
    except Exception as e:
        return "", f"é”™è¯¯çš„æ—¶é—´æ ¼å¼ï¼Œéœ€ä¸º YYYY-MM-DD HH:MM:SS -> {e}"

    start_time = issue_time - timedelta(minutes=window_min)
    end_time = issue_time + timedelta(minutes=window_min)

    matched = []
    if not os.path.exists(LOG_DIRECTORY):
        return "", f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {LOG_DIRECTORY}"

    # éå†æ—¥å¿—ç›®å½•ï¼Œå°è¯•è§£ææ¯ä¸€è¡Œçš„æ—¶é—´æˆ³
    for root, _, files in os.walk(LOG_DIRECTORY):
        for name in files:
            path = os.path.join(root, name)
            try:
                with open(path, 'r', encoding='utf-8', errors='ignore') as fh:
                    for line in fh:
                        ts = _parse_timestamp_from_line(line)
                        if ts and start_time <= ts <= end_time:
                            matched.append(f"{name} {ts.isoformat()} {line.strip()}")
                            if len(matched) >= max_lines:
                                break
            except Exception as e:
                # å¿½ç•¥å•ä¸ªæ–‡ä»¶è¯»å–é”™è¯¯
                print(f"âš ï¸ è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {path} -> {e}")
        if len(matched) >= max_lines:
            break

    # å›é€€ç­–ç•¥ï¼šå¦‚æœæ²¡æœ‰è§£æåˆ°æ—¶é—´æˆ³ï¼ŒæŒ‰æ—¥æœŸåŒ¹é…è¡Œæ–‡æœ¬
    if not matched:
        date_only = issue_time_str.split(' ')[0]
        for root, _, files in os.walk(LOG_DIRECTORY):
            for name in files:
                path = os.path.join(root, name)
                try:
                    with open(path, 'r', encoding='utf-8', errors='ignore') as fh:
                        for line in fh:
                            if date_only in line:
                                matched.append(f"{name} {line.strip()}")
                                if len(matched) >= max_lines:
                                    break
                except Exception:
                    continue
            if len(matched) >= max_lines:
                break

    return ("\n".join(matched), None)


def _build_prompt(description: str, issue_time: str, logs: str):
    return (
        "ä½ æ˜¯ç³»ç»Ÿæ—¥å¿—åˆ†æä¸“å®¶ã€‚\n"
        f"é—®é¢˜æ—¶é—´: {issue_time}\n"
        f"ç”¨æˆ·æè¿°: {description}\n"
        "è¯·é˜…è¯»ä¸‹é¢çš„æ—¥å¿—ç‰‡æ®µï¼Œåˆ†æå¯èƒ½çš„æ ¹å› ï¼ˆroot causeï¼‰ï¼Œæ ‡æ³¨å…³é”®æ—¥å¿—è¡Œï¼Œå¹¶ç»™å‡ºå¯æ‰§è¡Œçš„æ’æŸ¥å’Œä¿®å¤å»ºè®®ã€‚\n"
        "è¯·ç”¨ä¸­æ–‡ç»“æ„åŒ–è¾“å‡ºï¼ŒåŒ…å«ï¼šsummaryï¼ˆç®€è¦ç»“è®ºï¼‰ã€root_cause_hypothesisï¼ˆæ ¹å› å‡è®¾ï¼‰ã€key_log_linesï¼ˆå…³é”®æ—¥å¿—è¡Œä¸è§£é‡Šï¼‰ã€suggested_actionsï¼ˆå»ºè®®æ“ä½œï¼‰ã€‚\n\n"
        "ç›¸å…³æ—¥å¿—å¼€å§‹:\n" + (logs or "ï¼ˆæ— æ‰¾åˆ°ç›¸å…³æ—¥å¿—ï¼‰") + "\n\nè¯·å¼€å§‹åˆ†æï¼š"
    )


def call_ai_model(prompt: str, retries: int = 3, backoff: float = 2.0):
    """è°ƒç”¨å¤–éƒ¨å¤§æ¨¡å‹ APIï¼ˆOpenAI/å…¼å®¹ APIï¼‰ã€‚
    æ”¯æŒé‡è¯•ä¸æŒ‡æ•°é€€é¿ï¼Œè¿”å› dictï¼ˆåŒ…å« raw æ–‡æœ¬æˆ– errorï¼‰ã€‚
    """
    if not API_KEY:
        return {"error": "æœªé…ç½® API_KEYï¼ˆè¯·åœ¨ç¯å¢ƒå˜é‡æˆ– config.py ä¸­è®¾ç½®ï¼‰"}

    headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": API_MODEL or "gpt-3.5-turbo",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": MAX_TOKENS or 1000,
        "temperature": TEMPERATURE or 0.7
    }

    url = API_BASE_URL.rstrip('/') + '/chat/completions'
    last_err = None
    for attempt in range(1, retries + 1):
        try:
            resp = requests.post(url, headers=headers, json=payload, timeout=REQUEST_TIMEOUT or 30)
            resp.raise_for_status()
            data = resp.json()
            # å°è¯•è§£æç»“æœ
            choice = (data.get('choices') or [None])[0]
            if not choice:
                return {"error": "æ¨¡å‹è¿”å›æ ¼å¼å¼‚å¸¸", "raw": data}
            message = choice.get('message') or {}
            content = message.get('content') or choice.get('text') or ''
            return {"raw": content, "meta": data, "attempt": attempt}
        except requests.exceptions.RequestException as e:
            last_err = str(e)
            print(f"âš ï¸ è°ƒç”¨AIå¤±è´¥ (attempt {attempt}/{retries}): {last_err}")
            if attempt < retries:
                sleep_time = backoff * (2 ** (attempt - 1))
                print(f"   -> é‡è¯•ç­‰å¾… {sleep_time} ç§’...")
                time.sleep(sleep_time)
                continue
            else:
                return {"error": last_err}
        except Exception as e:
            return {"error": str(e)}


@app.route('/api/diagnose', methods=['POST'])
def diagnose_issue():
    """æ ¹æ®ç”¨æˆ·ç»™å®šçš„æ—¶é—´/æè¿°ï¼Œå®šä½æ—¥å¿—ç‰‡æ®µå¹¶è°ƒç”¨å¤§æ¨¡å‹è¿”å›è¯Šæ–­å»ºè®®ã€‚"""
    data = request.get_json() or {}
    issue_time = data.get('issue_time')
    description = data.get('description', '')
    window = int(data.get('window', 10))

    if not issue_time:
        return jsonify({"status": "error", "message": "ç¼ºå°‘å‚æ•° issue_timeï¼Œæ ¼å¼: YYYY-MM-DD HH:MM:SS"}), 400

    logs, err = extract_logs_around_time(issue_time, window)
    if err:
        return jsonify({"status": "error", "message": err}), 400

    prompt = _build_prompt(description, issue_time, logs)
    ai_result = call_ai_model(prompt)

    return jsonify({
        "status": "success",
        "issue_time": issue_time,
        "window_minutes": window,
        "logs_found": bool(logs),
        "logs_preview": logs[:4000],
        "ai_analysis": ai_result
    })


# ==================== æ™ºèƒ½Agentè¯Šæ–­ ====================

# å…¨å±€Agentå®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
_log_agent = None

def get_log_agent():
    """è·å–æˆ–åˆ›å»ºLogDiagnosticAgentå®ä¾‹"""
    global _log_agent
    if _log_agent is None:
        _log_agent = LogDiagnosticAgent(LOG_DIRECTORY)
    return _log_agent


@app.route('/api/agent/diagnose', methods=['POST'])
def agent_diagnose():
    """
    æ™ºèƒ½Agentè¯Šæ–­API
    
    Agentä¼šæ ¹æ®é—®é¢˜æè¿°æ™ºèƒ½é€‰æ‹©ç›¸å…³æ—¥å¿—æ–‡ä»¶è¿›è¡Œåˆ†æï¼Œ
    è€Œä¸æ˜¯ç›²ç›®è¯»å–æ‰€æœ‰æ—¥å¿—ã€‚
    
    è¯·æ±‚å‚æ•°:
    - description: é—®é¢˜æè¿°ï¼ˆå¿…éœ€ï¼‰
    - issue_time: é—®é¢˜å‘ç”Ÿæ—¶é—´ï¼ˆå¯é€‰ï¼Œæ ¼å¼: YYYY-MM-DD HH:MM:SSï¼‰
    - window: æ—¶é—´çª—å£ï¼ˆåˆ†é’Ÿï¼Œé»˜è®¤10ï¼‰
    - max_lines_per_file: æ¯ä¸ªæ—¥å¿—æ–‡ä»¶æœ€å¤§è¯»å–è¡Œæ•°ï¼ˆé»˜è®¤500ï¼‰
    
    è¿”å›:
    - reasoning: Agentçš„æ€è€ƒè¿‡ç¨‹
    - selected_logs: é€‰æ‹©çš„æ—¥å¿—æ–‡ä»¶åŠåŸå› 
    - log_contents: æå–çš„æ—¥å¿—å†…å®¹æ‘˜è¦
    - ai_analysis: AIåˆ†æç»“æœ
    """
    data = request.get_json() or {}
    description = data.get('description', '').strip()
    issue_time = data.get('issue_time', '')
    window = int(data.get('window', 10))
    max_lines = int(data.get('max_lines_per_file', 500))
    
    if not description:
        return jsonify({
            "status": "error",
            "message": "ç¼ºå°‘å‚æ•° descriptionï¼ˆé—®é¢˜æè¿°ï¼‰"
        }), 400
    
    try:
        agent = get_log_agent()
        result = agent.diagnose(
            problem_description=description,
            issue_time=issue_time if issue_time else None,
            window_minutes=window
        )
        
        return jsonify({
            "status": "success",
            **result
        })
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({
            "status": "error",
            "message": f"Agentè¯Šæ–­å¤±è´¥: {str(e)}"
        }), 500


@app.route('/api/agent/logs-info', methods=['GET'])
def agent_logs_info():
    """
    è·å–Agentçš„æ—¥å¿—çŸ¥è¯†åº“ä¿¡æ¯
    
    è¿”å›Agentäº†è§£çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ç±»å‹åŠå…¶ç”¨é€”è¯´æ˜
    """
    try:
        agent = get_log_agent()
        knowledge = agent.get_log_knowledge()
        
        return jsonify({
            "status": "success",
            "log_types_count": len(knowledge),
            "knowledge_base": knowledge
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"è·å–æ—¥å¿—çŸ¥è¯†åº“å¤±è´¥: {str(e)}"
        }), 500


@app.route('/api/agent/available-logs', methods=['GET'])
def agent_available_logs():
    """
    è·å–å½“å‰æ—¥å¿—ç›®å½•ä¸­å®é™…å­˜åœ¨çš„æ—¥å¿—æ–‡ä»¶
    
    è¿”å›æ¯ä¸ªæ—¥å¿—æ–‡ä»¶çš„åç§°ã€å¤§å°ã€ç±»å‹è¯´æ˜
    """
    try:
        agent = get_log_agent()
        available = agent.list_available_logs()
        
        return jsonify({
            "status": "success",
            "logs_count": len(available),
            "logs": available
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"è·å–å¯ç”¨æ—¥å¿—åˆ—è¡¨å¤±è´¥: {str(e)}"
        }), 500


# å‰ç«¯é™æ€æ–‡ä»¶æœåŠ¡è·¯ç”±
@app.route("/", methods=["GET"])
def serve_index():
    """æœåŠ¡å‰ç«¯ä¸»é¡µ"""
    return send_from_directory(FRONTEND_DIR, "index.html")

@app.route("/<path:filename>", methods=["GET"])
def serve_static_files(filename):
    """æœåŠ¡é™æ€æ–‡ä»¶ï¼ˆCSSã€JSç­‰ï¼‰"""
    return send_from_directory(FRONTEND_DIR, filename)

if __name__ == "__main__":
    app.run(port=8080, debug=False, threaded=False)